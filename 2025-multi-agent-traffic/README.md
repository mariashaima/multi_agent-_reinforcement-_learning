**ğŸš€ Key Features**

Multiâ€‘Agent Reinforcement Learning (MARL) setup with PPO

Adaptive adversarial traffic agents capable of:

Cutâ€‘in / cutâ€‘off maneuvers

Tailgating & forced merges

Rearâ€‘end collisions

Aggressive overtaking

IDMâ€‘controlled ego vehicle for baseline AV behavior

Procedurally generated road networks (straight roads, intersections, roundabouts, etc.)

Custom reward & termination functions for adversarial behavior shaping

Distributed training using Ray RLlib (CPU + GPU support)

Policy checkpointing & evaluation in both trained and unseen scenarios

Generalization tests across different map structures

**ğŸ“¦ Project Structure**
<img width="504" height="163" alt="image" src="https://github.com/user-attachments/assets/3ddebc57-5a4b-4b91-8b8a-274be2ab0d8b" />

**ğŸ§  Methodology Overview**

1. Environment Setup
Built on MetaDrive with procedural map generation

Ego vehicle uses Intelligent Driver Model (IDM)

Two nearest traffic vehicles are selected as MARL agents

Agents receive LiDARâ€‘like observations (72â€“240 dims)

2. Reward Design
Agents are rewarded for:

Reducing distance to the ego vehicle

Performing cutâ€‘ins, cutâ€‘offs, overtakes

Maintaining forward progress

Penalties for:

Collisions with nonâ€‘ego vehicles

Leaving the drivable area

Crashing into static objects

3. Training
PPO with clipped objective

Distributed rollout workers

50+ iterations Ã— 10 sessions

Best policy selected via reward convergence

4. Evaluation
Replaying trained policies in:

Straight roads

Roundabouts

Novel procedural maps

Measuring adversarial behavior consistency

Visualizing trajectories & interactions

**ğŸ› ï¸ Installation**
1. Clone the repository
bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>
2. Create environment
bash
conda create -n marl-traffic python=3.9
conda activate marl-traffic
3. Install dependencies
bash
pip install -r requirements.txt
Dependencies include:

MetaDrive

Ray + RLlib

PyTorch

NumPy / Pandas

Matplotlib / Seaborn

**â–¶ï¸ Training**
Run MARL training with PPO:

bash
python training/train_marl_agents.py --config configs/ppo_marl.yaml
This will:

Initialize MetaDrive multi-agent environment

Launch Ray rollout workers

Train AGENT1 & AGENT2 adversarial policies

Save checkpoints in policies/

**ğŸ¯ Evaluation**
Evaluate trained policies:

bash
python evaluation/eval_policies.py --checkpoint policies/best_policy/
You can enable rendering:

bash
--render True
**ğŸ“Š Results Summary**
Trained MARL agents successfully learned to:

##Perform rearâ€‘end collisions
Before training, the traffic agents exhibited mostly random or nonâ€‘targeted behavior and rarely produced consistent rearâ€‘end collisions with the ego vehicle.

![demo](https://github.com/user-attachments/assets/1a7552b1-4c78-40f8-9a3f-e6d7bc54d61a)

After training, the MARL agents learned to intentionally perform rearâ€‘end collisions by closing the gap aggressively, maintaining high relative speed, and exploiting the ego vehicleâ€™s conservative behavior.
![scenario_0](https://github.com/user-attachments/assets/f2805e94-2e17-4cd0-9054-2ec6dbee99fa)

**Execute cutâ€‘ins and cutâ€‘offs**
Before training, the traffic agents behaved randomly and were unable to perform structured lateral maneuvers. Lane changes occurred sporadically, without awareness of the ego vehicleâ€™s position or timing, resulting in unrealistic or nonâ€‘adversarial interactions.
![demo](https://github.com/user-attachments/assets/b2335013-72ff-4f23-a86a-536c24962c46)

After training, the MARL agents learned to execute deliberate and wellâ€‘timed cutâ€‘ins and cutâ€‘offs:
Cutâ€‘ins: Agents merge sharply into the ego vehicleâ€™s lane with minimal headway, forcing the ego vehicle to brake or adjust its trajectory.
Cutâ€‘offs: Agents accelerate, overtake, and then reâ€‘enter the lane directly in front of the ego vehicle, reducing timeâ€‘toâ€‘collision and creating a highâ€‘pressure scenario.

![scenario_0](https://github.com/user-attachments/assets/f96a0a76-83e6-46cc-9b7d-995ade6ee4b5)

Coordinate multiâ€‘agent maneuvers
Generalize to unseen roundabout scenarios

The ego vehicle (IDM) exhibited:

Hesitation under adversarial pressure

Limited ability to avoid rearâ€‘end threats

Reduced maneuverability in multiâ€‘agent traps

These results demonstrate the effectiveness of MARL for generating realistic, safetyâ€‘critical scenarios.

ğŸ“˜ Citing This Work
If you use this repository in academic work, please cite:

Code
Joy, Maria Shaima. 
"Development of learning-based multi-agent models for validating automated driving functions in adaptive traffic simulations."
Masterâ€™s Thesis, Karlsruhe University of Applied Sciences, 2025.
ğŸ¤ Contributing
Contributions are welcome!
Please open an issue or submit a pull request.

ğŸ“„ License
Specify your license here (MIT, Apache 2.0, etc.)
